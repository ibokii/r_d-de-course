{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89abb15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73680fc8-0537-441d-90b4-99b5a90e8c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "spark_context = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4192c35-9117-4c79-89b5-9f011f4bc68c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "366da265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark = SparkSession.builder.master(\"spark://spark-master:7077\").config(\"spark.jars.packages\", \n",
    "                                                                        #\"org.apache.hadoop:hadoop-aws-2.7.3\").appName(\"spark-example\").getOrCreate()\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2478e69a-c284-40cb-8b3c-d36bf4d0f49d",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a62f5b7-9ca6-4dbd-9949-de6ed83acfdc",
   "metadata": {},
   "source": [
    "### Load CSV file into DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05452853-8b07-4135-9727-31f2fefaaae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------------+-------------------+--------------------+---+-------------+----------+--------+-------+-------+\n",
      "|oscar_no|oscar_yr|       award|               name|               movie|age|     birth_pl|birth_date|birth_mo|birth_d|birth_y|\n",
      "+--------+--------+------------+-------------------+--------------------+---+-------------+----------+--------+-------+-------+\n",
      "|       1|    1929|Best actress|       Janet Gaynor|          7th Heaven| 22| Pennsylvania|1906-10-06|      10|      6|   1906|\n",
      "|       2|    1930|Best actress|      Mary Pickford|            Coquette| 37|       Canada|1892-04-08|       4|      8|   1892|\n",
      "|       3|    1931|Best actress|      Norma Shearer|        The Divorcee| 28|       Canada|1902-08-10|       8|     10|   1902|\n",
      "|       4|    1932|Best actress|     Marie Dressler|        Min and Bill| 63|       Canada|1868-11-09|      11|      9|   1868|\n",
      "|       5|    1933|Best actress|        Helen Hayes|The Sin of Madelo...| 32|Washington DC|1900-10-10|      10|     10|   1900|\n",
      "|       6|    1934|Best actress|  Katharine Hepburn|       Morning Glory| 26|  Connecticut|1907-05-12|       5|     12|   1907|\n",
      "|       7|    1935|Best actress|  Claudette Colbert|It Happened One N...| 31|       France|1903-09-13|       9|     13|   1903|\n",
      "|       8|    1936|Best actress|        Bette Davis|           Dangerous| 27|Massachusetts|1908-04-05|       4|      5|   1908|\n",
      "|       9|    1937|Best actress|       Luise Rainer|  The Great Zeigfeld| 26|      Germany|1910-01-12|       1|     12|   1910|\n",
      "|      10|    1938|Best actress|       Luise Rainer|      The Good Earth| 27|      Germany|1910-01-12|       1|     12|   1910|\n",
      "|      11|    1939|Best actress|        Bette Davis|             Jezebel| 30|Massachusetts|1908-04-05|       4|      5|   1908|\n",
      "|      12|    1940|Best actress|       Vivien Leigh|  Gone With the Wind| 26|        India|1913-11-05|      11|      5|   1913|\n",
      "|      13|    1941|Best actress|      Ginger Rogers|         Kitty Foyle| 29|     Missouri|1911-07-16|       7|     16|   1911|\n",
      "|      14|    1942|Best actress|      Joan Fontaine|           Suspicion| 24|        Japan|1917-10-22|      10|     22|   1917|\n",
      "|      15|    1943|Best actress|       Greer Garson|        Mrs. Miniver| 38|      England|1904-09-29|       9|     29|   1904|\n",
      "|      16|    1944|Best actress|     Jennifer Jones|The Song of Berna...| 24|     Oklahoma|1919-03-02|       3|      2|   1919|\n",
      "|      17|    1945|Best actress|     Ingrid Bergman|            Gaslight| 29|       Sweden|1915-08-29|       8|     29|   1915|\n",
      "|      18|    1946|Best actress|      Joan Crawford|      Mildred Pierce| 33|        Texas|1912-03-23|       3|     23|   1912|\n",
      "|      19|    1947|Best actress|Olivia de Havilland|     To Each His Own| 30|        Japan|1916-07-01|       7|      1|   1916|\n",
      "|      20|    1948|Best actress|      Loretta Young|The Farmer's Daug...| 34|         Utah|1913-01-06|       1|      6|   1913|\n",
      "+--------+--------+------------+-------------------+--------------------+---+-------------+----------+--------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_file = \"../data/oscars.csv\"\n",
    "df_csv = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
    "\n",
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1edc357a-b30c-4edb-be92-92ebea0f482b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Relation [oscar_no#17,oscar_yr#18,award#19,name#20,movie#21,age#22,birth_pl#23,birth_date#24,birth_mo#25,birth_d#26,birth_y#27] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "oscar_no: int, oscar_yr: int, award: string, name: string, movie: string, age: int, birth_pl: string, birth_date: date, birth_mo: int, birth_d: int, birth_y: int\n",
      "Relation [oscar_no#17,oscar_yr#18,award#19,name#20,movie#21,age#22,birth_pl#23,birth_date#24,birth_mo#25,birth_d#26,birth_y#27] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Relation [oscar_no#17,oscar_yr#18,award#19,name#20,movie#21,age#22,birth_pl#23,birth_date#24,birth_mo#25,birth_d#26,birth_y#27] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "FileScan csv [oscar_no#17,oscar_yr#18,award#19,name#20,movie#21,age#22,birth_pl#23,birth_date#24,birth_mo#25,birth_d#26,birth_y#27] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/ihor/dev/repos/r_d-de-course/lec14/data/oscars.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<oscar_no:int,oscar_yr:int,award:string,name:string,movie:string,age:int,birth_pl:string,bi...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdf80370-7e83-4734-86bb-a0cd26820e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- oscar_no: integer (nullable = true)\n",
      " |-- oscar_yr: integer (nullable = true)\n",
      " |-- award: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- movie: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- birth_pl: string (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- birth_mo: integer (nullable = true)\n",
      " |-- birth_d: integer (nullable = true)\n",
      " |-- birth_y: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5be9330c-5630-4a76-b748-628bce817d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "| id|year| age|\n",
      "+---+----+----+\n",
      "|  1|1929|NULL|\n",
      "|  2|1930|NULL|\n",
      "|  3|1931|NULL|\n",
      "|  4|1932|NULL|\n",
      "|  5|1933|NULL|\n",
      "|  6|1934|NULL|\n",
      "|  7|1935|NULL|\n",
      "|  8|1936|NULL|\n",
      "|  9|1937|NULL|\n",
      "| 10|1938|NULL|\n",
      "| 11|1939|NULL|\n",
      "| 12|1940|NULL|\n",
      "| 13|1941|NULL|\n",
      "| 14|1942|NULL|\n",
      "| 15|1943|NULL|\n",
      "| 16|1944|NULL|\n",
      "| 17|1945|NULL|\n",
      "| 18|1946|NULL|\n",
      "| 19|1947|NULL|\n",
      "| 20|1948|NULL|\n",
      "+---+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True) #Wrong type\n",
    "])\n",
    "\n",
    "df_csv_with_schema = spark.read.schema(schema).csv(csv_file, header=True)\n",
    "\n",
    "df_csv_with_schema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f02325f4-18ba-476f-a941-1a3183e09073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv_with_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ad35e4f-997e-45e2-a611-b2bdbdc11155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------------+-------------------+--------------------+---+-------------+----------+--------+-------+-------+\n",
      "|oscar_no|oscar_yr|       award|               name|               movie|age|     birth_pl|birth_date|birth_mo|birth_d|birth_y|\n",
      "+--------+--------+------------+-------------------+--------------------+---+-------------+----------+--------+-------+-------+\n",
      "|       1|    1929|Best actress|       Janet Gaynor|          7th Heaven| 22| Pennsylvania|1906-10-06|      10|      6|   1906|\n",
      "|       2|    1930|Best actress|      Mary Pickford|            Coquette| 37|       Canada|1892-04-08|       4|      8|   1892|\n",
      "|       3|    1931|Best actress|      Norma Shearer|        The Divorcee| 28|       Canada|1902-08-10|       8|     10|   1902|\n",
      "|       4|    1932|Best actress|     Marie Dressler|        Min and Bill| 63|       Canada|1868-11-09|      11|      9|   1868|\n",
      "|       5|    1933|Best actress|        Helen Hayes|The Sin of Madelo...| 32|Washington DC|1900-10-10|      10|     10|   1900|\n",
      "|       6|    1934|Best actress|  Katharine Hepburn|       Morning Glory| 26|  Connecticut|1907-05-12|       5|     12|   1907|\n",
      "|       7|    1935|Best actress|  Claudette Colbert|It Happened One N...| 31|       France|1903-09-13|       9|     13|   1903|\n",
      "|       8|    1936|Best actress|        Bette Davis|           Dangerous| 27|Massachusetts|1908-04-05|       4|      5|   1908|\n",
      "|       9|    1937|Best actress|       Luise Rainer|  The Great Zeigfeld| 26|      Germany|1910-01-12|       1|     12|   1910|\n",
      "|      10|    1938|Best actress|       Luise Rainer|      The Good Earth| 27|      Germany|1910-01-12|       1|     12|   1910|\n",
      "|      11|    1939|Best actress|        Bette Davis|             Jezebel| 30|Massachusetts|1908-04-05|       4|      5|   1908|\n",
      "|      12|    1940|Best actress|       Vivien Leigh|  Gone With the Wind| 26|        India|1913-11-05|      11|      5|   1913|\n",
      "|      13|    1941|Best actress|      Ginger Rogers|         Kitty Foyle| 29|     Missouri|1911-07-16|       7|     16|   1911|\n",
      "|      14|    1942|Best actress|      Joan Fontaine|           Suspicion| 24|        Japan|1917-10-22|      10|     22|   1917|\n",
      "|      15|    1943|Best actress|       Greer Garson|        Mrs. Miniver| 38|      England|1904-09-29|       9|     29|   1904|\n",
      "|      16|    1944|Best actress|     Jennifer Jones|The Song of Berna...| 24|     Oklahoma|1919-03-02|       3|      2|   1919|\n",
      "|      17|    1945|Best actress|     Ingrid Bergman|            Gaslight| 29|       Sweden|1915-08-29|       8|     29|   1915|\n",
      "|      18|    1946|Best actress|      Joan Crawford|      Mildred Pierce| 33|        Texas|1912-03-23|       3|     23|   1912|\n",
      "|      19|    1947|Best actress|Olivia de Havilland|     To Each His Own| 30|        Japan|1916-07-01|       7|      1|   1916|\n",
      "|      20|    1948|Best actress|      Loretta Young|The Farmer's Daug...| 34|         Utah|1913-01-06|       1|      6|   1913|\n",
      "+--------+--------+------------+-------------------+--------------------+---+-------------+----------+--------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load CSV file with specific options\n",
    "df_csv_options = spark.read.options(header='True', \n",
    "                                    inferSchema='True', \n",
    "                                    delimiter=',',\n",
    "                                    quote='\"',\n",
    "                                    dateFormat='yyyy-MM-dd',\n",
    "                                    escape='\\\\').csv(csv_file)\n",
    "\n",
    "df_csv_options.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903f6151-724c-4d70-a569-38a623585f75",
   "metadata": {},
   "source": [
    "### Load JSON file into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ca6e06f-b17f-44d1-b734-51cd0d5f02bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+--------------+-------------+\n",
      "|           client|price|       product|purchase_date|\n",
      "+-----------------+-----+--------------+-------------+\n",
      "|     Norma Fisher|  121|Vacuum cleaner|   2022-08-09|\n",
      "|     Norma Fisher|  348|Microwave oven|   2022-08-13|\n",
      "|     Norma Fisher| 1126|         Phone|   2022-08-12|\n",
      "|   Jorge Sullivan|  171|Microwave oven|   2022-08-10|\n",
      "|  Elizabeth Woods| 1766|            TV|   2022-08-26|\n",
      "|     Susan Wagner|  461|Microwave oven|   2022-08-26|\n",
      "|     Susan Wagner|  561|Microwave oven|   2022-08-05|\n",
      "| Peter Montgomery| 1994|            TV|   2022-08-03|\n",
      "| Peter Montgomery| 2804|coffee machine|   2022-08-16|\n",
      "|Stephanie Collins|  403|Vacuum cleaner|   2022-08-20|\n",
      "|Stephanie Collins| 1775|coffee machine|   2022-08-18|\n",
      "| Stephanie Sutton|  613|Vacuum cleaner|   2022-08-09|\n",
      "| Stephanie Sutton| 2148|            TV|   2022-08-30|\n",
      "| Stephanie Sutton|  568|Microwave oven|   2022-08-01|\n",
      "|       Susan Levy|  109|coffee machine|   2022-08-20|\n",
      "|       Susan Levy|  421|Vacuum cleaner|   2022-08-08|\n",
      "|       Susan Levy| 2764|coffee machine|   2022-08-23|\n",
      "|   Kimberly Smith|  213|Microwave oven|   2022-08-08|\n",
      "| Jennifer Summers|  329|Microwave oven|   2022-08-03|\n",
      "|      Dana Nguyen|  577|Microwave oven|   2022-08-16|\n",
      "+-----------------+-----+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_file = \"../data/sales.json\"\n",
    "df_json = spark.read.json(json_file, multiLine=True)\n",
    "\n",
    "df_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0f4417f-6ca9-4a25-acba-d97455e22945",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"details\", StructType([\n",
    "        StructField(\"age\", IntegerType(), True),\n",
    "        StructField(\"address\", StructType([\n",
    "            StructField(\"city\", StringType(), True),\n",
    "            StructField(\"state\", StringType(), True)\n",
    "        ]), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "nested_json_file = \"../data/few_files/2.json\"\n",
    "df_nested_json = spark.read.schema(schema).json(nested_json_file, multiLine=True)\n",
    "\n",
    "# df_nested_json.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28d5630e-b15b-4eb7-9350-aa71d38155b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-----------------------+\n",
      "|id |name           |details                |\n",
      "+---+---------------+-----------------------+\n",
      "|1  |John Doe       |{30, {New York, NY}}   |\n",
      "|2  |Jane Smith     |{25, {Los Angeles, CA}}|\n",
      "|3  |Michael Johnson|{40, {Chicago, IL}}    |\n",
      "+---+---------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "files = sorted(Path(\"../data/few_files/\").glob(\"*.json\"))\n",
    "dfs = [spark.read.schema(schema).json(str(f), multiLine=True) for f in files]\n",
    "df_all = reduce(DataFrame.unionByName, dfs)\n",
    "\n",
    "df_all.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "958240e4-9479-4d2f-998b-575424c8c2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- details: struct (nullable = true)\n",
      " |    |-- age: integer (nullable = true)\n",
      " |    |-- address: struct (nullable = true)\n",
      " |    |    |-- city: string (nullable = true)\n",
      " |    |    |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nested_json.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a6df3f-9b78-456c-893c-9f650e869d44",
   "metadata": {},
   "source": [
    "### Load Parquet file into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9898a232-f5b7-4835-a7a4-e60b5b83ce32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|NULL|    0|    0|          330877| 8.4583| NULL|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| NULL|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| NULL|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| NULL|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| NULL|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| NULL|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| NULL|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| NULL|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| NULL|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|NULL|    0|    0|          244373|   13.0| NULL|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| NULL|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|NULL|    0|    0|            2649|  7.225| NULL|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquet_file = \"../data/titanic.parquet\"\n",
    "df_parquet = spark.read.parquet(parquet_file)\n",
    "\n",
    "# Show DataFrame\n",
    "df_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586e46d2-79bf-4f32-bf01-15ff75cf43b3",
   "metadata": {},
   "source": [
    "### Load Text file into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "077e17d5-0885-4467-9e34-b882b7d857b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|03-17 16:13:38.81...|\n",
      "|03-17 16:13:38.81...|\n",
      "|03-17 16:13:38.82...|\n",
      "|03-17 16:13:38.83...|\n",
      "|03-17 16:13:38.85...|\n",
      "|03-17 16:13:38.86...|\n",
      "|03-17 16:13:38.86...|\n",
      "|03-17 16:13:38.87...|\n",
      "|03-17 16:13:38.87...|\n",
      "|03-17 16:13:38.87...|\n",
      "|03-17 16:13:38.88...|\n",
      "|03-17 16:13:38.88...|\n",
      "|03-17 16:13:38.88...|\n",
      "|03-17 16:13:38.88...|\n",
      "|03-17 16:13:38.90...|\n",
      "|03-17 16:13:38.90...|\n",
      "|03-17 16:13:38.91...|\n",
      "|03-17 16:13:38.92...|\n",
      "|03-17 16:13:38.92...|\n",
      "|03-17 16:13:38.93...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_file = \"../data/android.txt\"\n",
    "df_text = spark.read.text(text_file)\n",
    "\n",
    "# Show DataFrame\n",
    "df_text.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3134367-38d6-4d2e-aa00-bb929002aa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "# Load text file into an RDD\n",
    "rdd = sc.textFile(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7d54c08-ecd7-440e-b679-ea31bdbfeb18",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 33) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:713)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:757)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:713)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:757)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Didn't work\u001b[39;00m\n",
      "File \u001b[1;32m~\\dev\\repos\\r_d-de-course\\lec14\\pyspark-env\\lib\\site-packages\\pyspark\\rdd.py:2855\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   2852\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2854\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[1;32m-> 2855\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2857\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m   2858\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[1;32m~\\dev\\repos\\r_d-de-course\\lec14\\pyspark-env\\lib\\site-packages\\pyspark\\context.py:2510\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   2508\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m   2509\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2510\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32m~\\dev\\repos\\r_d-de-course\\lec14\\pyspark-env\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\dev\\repos\\r_d-de-course\\lec14\\pyspark-env\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\dev\\repos\\r_d-de-course\\lec14\\pyspark-env\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 33) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:713)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:757)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:713)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:757)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n"
     ]
    }
   ],
   "source": [
    "rdd.take(10) # Didn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "513b6801-f964-4ab7-aa04-e68334b3eca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                                                                                                                                                                                                         |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|03-17 16:13:38.811  1702  2395 D WindowManager: printFreezingDisplayLogsopening app wtoken = AppWindowToken{9f4ef63 token=Token{a64f992 ActivityRecord{de9231d u0 com.tencent.qt.qtl/.activity.info.NewsDetailXmlActivity t761}}}, allDrawn= false, startingDisplayed =  false, startingMoved =  false, isRelaunching =  false|\n",
      "|03-17 16:13:38.819  1702  8671 D PowerManagerService: acquire lock=233570404, flags=0x1, tag=\"View Lock\", name=com.android.systemui, ws=null, uid=10037, pid=2227                                                                                                                                                             |\n",
      "|03-17 16:13:38.820  1702  8671 D PowerManagerService: ready=true,policy=3,wakefulness=1,wksummary=0x23,uasummary=0x1,bootcompleted=true,boostinprogress=false,waitmodeenable=false,mode=false,manual=38,auto=-1,adj=0.0userId=0                                                                                               |\n",
      "|03-17 16:13:38.839  1702  2113 V WindowManager: Skipping AppWindowToken{df0798e token=Token{78af589 ActivityRecord{3b04890 u0 com.tencent.qt.qtl/com.tencent.video.player.activity.PlayerActivity t761}}} -- going to hide                                                                                                    |\n",
      "|03-17 16:13:38.859  2227  2227 D TextView: visible is system.time.showampm                                                                                                                                                                                                                                                    |\n",
      "|03-17 16:13:38.861  2227  2227 D TextView: mVisiblity.getValue is false                                                                                                                                                                                                                                                       |\n",
      "|03-17 16:13:38.869  2227  2227 D TextView: visible is system.charge.show                                                                                                                                                                                                                                                      |\n",
      "|03-17 16:13:38.871  2227  2227 D TextView: mVisiblity.getValue is false                                                                                                                                                                                                                                                       |\n",
      "|03-17 16:13:38.875  2227  2227 D TextView: visible is system.call.count gt 0                                                                                                                                                                                                                                                  |\n",
      "|03-17 16:13:38.877  2227  2227 D TextView: mVisiblity.getValue is false                                                                                                                                                                                                                                                       |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Used DataFrame API instead of RDD\n",
    "df_text = spark.read.text(text_file)\n",
    "df_text.show(10, truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45832318-e89a-4301-a16d-389ab9236fa2",
   "metadata": {},
   "source": [
    "### Loading Data from Apache Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d452b2b-d577-4af0-b5c3-f54fd7affb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session with Kafka package\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Load Kafka\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load data from Kafka\n",
    "kafka_df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"your_topic\") \\\n",
    "    .load()\n",
    "\n",
    "kafka_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8026d8-0496-4f64-9b8a-87c77f1a2174",
   "metadata": {},
   "source": [
    "### Loading Data from a JDBC Source with Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b654335-74c6-4895-8f0d-7fb1623a48be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from a SQL database with partitioning\n",
    "jdbc_url = \"jdbc:postgresql://your-db-host:5432/your-database\"\n",
    "properties = {\n",
    "    \"user\": \"your-username\",\n",
    "    \"password\": \"your-password\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "df_sql_partitioned = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"your_table_name\",\n",
    "    properties=properties,\n",
    "    column=\"id\",\n",
    "    lowerBound=1,\n",
    "    upperBound=100000,\n",
    "    numPartitions=10\n",
    ")\n",
    "\n",
    "df_sql_partitioned.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4bd585-3691-412a-bcf3-4f4c1b571909",
   "metadata": {},
   "source": [
    "### RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661cec89-bfac-43a4-acd8-732392344437",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6409d7-f248-476f-a374-286bae9cd7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_count = rdd.count()\n",
    "print(f\"Number of lines: {line_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bd12b4-03c6-4b6e-80c0-2bf695ef6c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_line = rdd.first()\n",
    "print(f\"First line: {first_line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797dd20e-5c05-4122-a38f-0b8304334e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_rdd = rdd.filter(lambda line: 'WindowManager:' in line)\n",
    "filtered_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0e630f-05f3-4fc1-8ffb-2b30230a6932",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_lengths = rdd.map(lambda line: len(line))\n",
    "line_lengths.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d4eb58-0368-46bc-b6e1-f1b555b0f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to get the total number of characters in the text file\n",
    "total_characters = line_lengths.reduce(lambda a, b: a + b)\n",
    "print(f\"Total number of characters: {total_characters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2497cf4-1bef-4ac6-bf28-71da664a0e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.flatMap(lambda line: line.split(\" \")).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460e8ef3-b5e8-41ad-9c96-9a2ef336861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = rdd.flatMap(lambda line: line.split(\" \"))\n",
    "word_pairs = words.map(lambda word: (word, 1))\n",
    "word_counts = word_pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "word_counts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71719bb5-58b1-4330-a3f6-152bbbe8748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"word\", StringType(), True),\n",
    "    StructField(\"count\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "word_counts_df = spark.createDataFrame(word_counts, schema).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142584f5-552b-4e79-a344-3c308562859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4104b181-97d8-43de-bea4-c30243ae865d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regular expression pattern for real words\n",
    "pattern = re.compile(r'\\b[A-Za-z]+\\b')\n",
    "\n",
    "def extract_words(line):\n",
    "    return pattern.findall(line)\n",
    "\n",
    "words_rdd = rdd.flatMap(extract_words)\n",
    "\n",
    "words_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb72c69a-a6a8-4ad3-a9d0-02bc9242d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = words_rdd.map(lambda word: (word, 1))\n",
    "word_counts = word_pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "word_counts.sortByKey().take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9cd2a9-f878-40a9-aad1-fb8f1c42c308",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.sortBy(lambda x: x[1], ascending=False).take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea51e50f-80f7-4c8a-b0e8-5a4356123658",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.distinct().take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe9c6a5-35a0-42be-bde0-37fd74fcc943",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d8c9f5-0cf3-446e-abf5-e5f8a931f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.collect()\n",
    "\n",
    "word_counts.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97f3c17-9f74-40e5-8db0-d928c5205a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 2)])\n",
    "rdd2 = sc.parallelize([(\"a\", 3), (\"b\", 4), (\"c\", 5)])\n",
    "result = rdd1.join(rdd2)\n",
    "result.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a10cd4-73b6-4ec0-9821-5b632736c05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([1, 2, 3])\n",
    "rdd2 = sc.parallelize([3, 4, 5])\n",
    "result = rdd1.union(rdd2)\n",
    "\n",
    "result.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed5b986-4cfe-4b2c-9c39-aaf77ca23e62",
   "metadata": {},
   "source": [
    "### DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa7250b-37e0-4a81-a695-1512dac6231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.select(\"name\").show()\n",
    "df_parquet.select(\"name\", \"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82dab7d-3596-4ee6-8501-f87460699375",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.filter(df_parquet[\"age\"] > 25).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a50fbcc-2f63-48c9-a711-a4aea8977bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.filter(\"age > 25\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b19a66-b4a4-44db-a06b-91a783113d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.filter(F.col(\"age\") > 25).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b860ea6a-6e85-454b-a9e1-5069214deea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.filter((F.col(\"age\") > 25) & (F.col(\"age\") < 30)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d219971f-b5f2-44ee-b819-8f3a7b1a34d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df_parquet.groupBy(\"age\").count()\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bef5dc4-5dc9-4a4e-9759-edb87be5ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.groupBy(\"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08101e6c-cb82-439d-b461-c41cd2216426",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df_parquet.groupBy(\"age\").avg(\"Fare\")\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f7db2-bd37-4573-839b-493d3f5d74e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.agg(F.avg(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae5edb0-69d3-4210-b0e4-1d6ff1662c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.agg(F.min(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ba58e3-cde7-430c-b739-ef7a060e40d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.agg(F.max(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d229d0-cf6d-4218-a69b-8a948d4b8c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.withColumn(\"age_in_10_years\", F.col(\"age\") + 10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6745011-e06e-4919-be8a-7c423627eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"John\", 30), (\"Jane\", 25), (\"Doe\", 22)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
    "\n",
    "data2 = [(\"John\", \"USA\"), (\"Jane\", \"UK\"), (\"Doe\", \"Canada\")]\n",
    "df2 = spark.createDataFrame(data2, [\"name\", \"country\"])\n",
    "\n",
    "df_joined = df.join(df2, on=\"name\", how=\"inner\")\n",
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3ee62b-929d-482e-adf1-cea95a78da3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.orderBy(\"age\").show()\n",
    "df_parquet.orderBy(F.col(\"age\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6da56f-f11b-44b5-959b-9f2ff9e1f042",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.drop(\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e7606-f164-499d-b491-a2b6b81a7211",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d821a84-82dd-4539-aa85-ca5643d02424",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4af89af-4f94-459f-8952-4106d3a6a533",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef05dcb8-a176-4938-9642-cb5f0f25b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.dropDuplicates([\"age\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac18dd01-1706-4b22-a60d-a07432faa567",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.fillna({\"age\": 0}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed3435f-0741-4bf2-baef-38a58e144c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.replace({3: 30, 1: 10}, subset='Pclass').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a0fc27-9a57-41b7-aa1c-b28bb9416d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.sample(fraction=0.5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea43bfc-5491-420d-8b4f-d5c9b4e5e5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [(\"John\", 30), (\"Jane\", 25)]\n",
    "data2 = [(\"John\", 30), (\"Doe\", 22)]\n",
    "df1 = spark.createDataFrame(data1, [\"name\", \"age\"])\n",
    "df2 = spark.createDataFrame(data2, [\"name\", \"age\"])\n",
    "\n",
    "df1.intersect(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6274ad2-46ae-433c-8949-5a43ca733793",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [(\"John\", 30), (\"Jane\", 25)]\n",
    "data2 = [(\"Doe\", 22), (\"Smith\", 35)]\n",
    "df1 = spark.createDataFrame(data1, [\"name\", \"age\"])\n",
    "df2 = spark.createDataFrame(data2, [\"name\", \"age\"])\n",
    "\n",
    "df1.union(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5050e4c-5422-4b14-99d4-1a68b484abe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [(\"John\", 30), (\"Jane\", 25)]\n",
    "data2 = [(\"John\", 30), (\"Doe\", 22)]\n",
    "df1 = spark.createDataFrame(data1, [\"name\", \"age\"])\n",
    "df2 = spark.createDataFrame(data2, [\"name\", \"age\"])\n",
    "\n",
    "df1.exceptAll(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b27e51e-fbb7-4a3c-b500-f783c99f2c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30f622-cf37-4eb5-a5e7-8e19e0a04e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.select(df_parquet[\"name\"].alias(\"full_name\"), \"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3656dc21-365e-442b-ba7f-5dbf527053bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = df_parquet.toPandas()\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b71101-9e6c-4068-a1f0-86c18b9685c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.withColumnRenamed(\"name\", \"full_name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2edbb8-f64a-498e-8459-4625fceb4f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.sort(df_parquet[\"age\"].desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e9829d-7849-4fb8-b410-2af21b3845f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.rdd.take(\n",
    "    10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cd3f54-db39-40b0-a47e-a2da052b3e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360928bf-9978-4619-a349-0983120f7d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_parquet.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e81b7af-f662-403e-8f67-431f426c3b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in df_parquet.randomSplit([0.3, 0.4, 0.3]):\n",
    "    part.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf92eb-d43e-45e0-bdc5-d2e289415c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.write.mode(\"overwrite\").parquet(\"./data/output/new_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e2e422-41a0-4121-8077-4960f3e98eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.filter(F.col(\"age\").isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc86af7-a60a-4ba6-a70a-102ed29af4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.filter(F.col(\"age\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa43a4-dcbc-428e-9e23-4dc411c0c1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.dropna().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a17cd9-7ef7-4e7a-a619-1e09ff728227",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.select(F.countDistinct(\"name\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760b9008-25a0-480d-adf7-a74aa9382df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.corr(\"age\", \"Fare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ca33c8-40d2-40a0-9cb8-0b3f52a726f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.cov(\"age\", \"Fare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9008fa-e683-4053-9034-b1ba7ea30144",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = df_parquet.approxQuantile(\"age\", [0.25, 0.5, 0.75], 0.01)\n",
    "print(f\"Approximate quantiles: {quantiles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b715c3-8a7d-45a2-ae14-b3c8e321fdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.sampleBy(\"name\", fractions={\"a\": 0.5, \"b\": 1.0, \"c\": 0.2}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb121f-d80b-4d37-b025-552804470d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.rollup(\"age\").sum(\"Fare\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f2d9b5-1770-45dc-a539-1ab9e8579f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_years(df):\n",
    "    return df.withColumn(\"age_plus_5\", F.col(\"age\") + 5)\n",
    "    \n",
    "df_parquet.transform(add_years).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e9acd-e29a-4856-a33a-72c49fdf1b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [(\"John\", 30), (\"Jane\", 25)]\n",
    "data2 = [(\"USA\",), (\"UK\",)]\n",
    "df1 = spark.createDataFrame(data1, [\"name\", \"age\"])\n",
    "df2 = spark.createDataFrame(data2, [\"country\"])\n",
    "\n",
    "# Perform a cross join\n",
    "df_cross_join = df1.crossJoin(df2)\n",
    "df_cross_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d02174c-ee2e-4d62-b2f2-aa2da44305b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Run SQL queries on the temporary view\n",
    "spark.sql(\"SELECT * FROM people WHERE age > 25\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8c68df-7039-4746-9f6e-e95369d5f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.selectExpr(\"name as full_name\", \"age + 5 as age_in_5_years\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a2f606-1f98-4352-8128-d0e6cdce597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.inputFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e57593-bfe6-430b-96eb-2747f5c39a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.isEmpty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e3c150-4a7b-4d51-b6d5-56d9821144de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab267e1-2fd4-4c48-b067-57b3a946a9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cadddbf-86cb-45e1-9f85-01e2aaaeeefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"sex\").orderBy(\"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fc175e-8e66-484b-810d-f56ec7984925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply row_number\n",
    "df_parquet.withColumn(\"row_number\", F.row_number().over(window_spec)).where(\"row_number = 1\").show()\n",
    "\n",
    "# Apply rank\n",
    "df_parquet.withColumn(\"rank\", F.rank().over(window_spec)).where(\"rank = 1\").show()\n",
    "\n",
    "# Apply dense_rank\n",
    "df_parquet.withColumn(\"dense_rank\", F.dense_rank().over(window_spec)).where(F.col(\"dense_rank\").isin([1, 2])).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e144bab1-79b6-4679-a267-aa341577cf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter names that contain the substring 'a'\n",
    "df_parquet.filter(F.col(\"name\").like(\"%a%\")).show()\n",
    "\n",
    "# Filter names that start with 'A'\n",
    "df_parquet.filter(F.col(\"name\").like(\"A%\")).show()\n",
    "\n",
    "# Filter names that end with 'e'\n",
    "df_parquet.filter(F.col(\"name\").like(\"%e\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f96cec-7c22-4002-a29a-e7586b83acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.filter(F.col(\"name\").endswith(\"A\")).show()\n",
    "\n",
    "df_parquet.filter(F.col(\"name\").startswith(\"D\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac273031-e4ae-4532-ac55-dc6530d7f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92297ec4-384e-4dab-81ad-8bc0514739ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
